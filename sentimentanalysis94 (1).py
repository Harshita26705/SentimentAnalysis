# -*- coding: utf-8 -*-
"""SentimentAnalysis94.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HmNJ3EeoVdi8Ax0-EGD3sxFeOV4t76F5
"""



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('Reviews.csv', on_bad_lines='skip', encoding='utf-8')
print("✅ Shape of data:", df.shape)
df.head()

df.head()

df.info()

df.shape

df.isnull().sum()

# Keep only necessary columns
df = df[['Text', 'Score']]

# Drop rows with missing values in 'Text' or 'Score'
df.dropna(subset=['Text', 'Score'], inplace=True)

# Confirm no missing values remain in these columns
print(df.isnull().sum())

def get_sentiment(score):
    if score in [1, 2]:
        return 'negative'
    elif score == 3:
        return 'neutral'
    else:
        return 'positive'

df['Sentiment'] = df['Score'].apply(get_sentiment)

# Count the sentiment classes
df['Sentiment'].value_counts().plot(kind='bar', color=['red', 'gray', 'green'])
plt.title('Sentiment Class Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Number of Reviews')
plt.xticks(rotation=0)
plt.show()

# Or just print counts
print(df['Sentiment'].value_counts())

from sklearn.utils import resample

# Separate classes
positive = df[df['Sentiment'] == 'positive']
negative = df[df['Sentiment'] == 'negative']
neutral = df[df['Sentiment'] == 'neutral']

# Downsample positive to match the number of neutral reviews
positive_downsampled = resample(positive, replace=False, n_samples=54557, random_state=42)
negative_downsampled = resample(negative, replace=False, n_samples=54557, random_state=42)

# Combine balanced data
df_balanced = pd.concat([positive_downsampled, negative_downsampled, neutral])

# Shuffle the dataset
df_balanced = df_balanced.sample(frac=1).reset_index(drop=True)

# Check balance again
print(df_balanced['Sentiment'].value_counts())

# Count the sentiment classes
df_balanced['Sentiment'].value_counts().plot(kind='bar', color=['red', 'gray', 'green'])
plt.title('Sentiment Class Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Number of Reviews')
plt.xticks(rotation=0)
plt.show()

# Or just print counts
print(df_balanced['Sentiment'].value_counts())

# Upsample all classes to match the largest class
max_samples = max(len(positive), len(negative), len(neutral))

positive_upsampled = resample(positive, replace=True, n_samples=max_samples, random_state=42)
neutral_upsampled  = resample(neutral,  replace=True, n_samples=max_samples, random_state=42)
negative_upsampled = resample(negative, replace=True, n_samples=max_samples, random_state=42)

# Combine and shuffle
df_balanced = pd.concat([positive_upsampled, neutral_upsampled, negative_upsampled])
df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)

# Count the sentiment classes
df_balanced['Sentiment'].value_counts().plot(kind='bar', color=['red', 'gray', 'green'])
plt.title('Sentiment Class Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Number of Reviews')
plt.xticks(rotation=0)
plt.show()

# Or just print counts
print(df_balanced['Sentiment'].value_counts())

df.describe()

import re

def clean_text(text):
    text = re.sub(r'<.*?>', '', text)           # Remove HTML tags
    text = re.sub(r'[^a-zA-Z ]', '', text)      # Remove numbers & punctuation
    text = text.lower()                         # Lowercase
    return text

# Apply cleaning
df_balanced['Clean_Text'] = df_balanced['Text'].apply(clean_text)

# Preview cleaned text
df_balanced[['Text', 'Clean_Text']].head()

from sklearn.feature_extraction.text import TfidfVectorizer

X = df_balanced['Clean_Text']
y = df_balanced['Sentiment']

# Convert text to vectors
vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
X_vec = vectorizer.fit_transform(X)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

# Train the model
model = MultinomialNB()
model.fit(X_train, y_train)

# Predict on test data
y_pred = model.predict(X_test)

# Accuracy score
print("✅ Accuracy:", accuracy_score(y_test, y_pred))

# Classification report
print("\n✅ Classification Report:\n", classification_report(y_test, y_pred))

# Confusion matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred, labels=["positive", "neutral", "negative"])
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["positive", "neutral", "negative"], yticklabels=["positive", "neutral", "negative"])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

print("✅ Accuracy:", accuracy_score(y_test, y_pred))

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import re

# Download NLTK stopwords (only once)
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()

def clean_and_stem(text):
    text = re.sub(r'[^a-zA-Z ]', '', text)  # Remove numbers/special characters
    words = text.lower().split()
    words = [stemmer.stem(word) for word in words if word not in stop_words]
    return " ".join(words)

df_balanced['Clean_Text'] = df_balanced['Text'].apply(clean_and_stem)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

X = df_balanced['Clean_Text']
y = df_balanced['Sentiment']

# TF-IDF
vectorizer = TfidfVectorizer(ngram_range=(1, 2))

X_vec = vectorizer.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)

# Naive Bayes training
from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

print("✅ Accuracy:", accuracy_score(y_test, y_pred))
print("✅ Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred, labels=['positive', 'neutral', 'negative'])
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['positive', 'neutral', 'negative'], yticklabels=['positive', 'neutral', 'negative'])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# 🔍 Manual Review Sentiment Prediction

def predict_review_sentiment(review_text):
    # Clean and stem input just like training data
    text = re.sub(r'[^a-zA-Z ]', '', review_text)
    words = text.lower().split()
    words = [stemmer.stem(word) for word in words if word not in stop_words]
    clean_text = " ".join(words)

    # Vectorize and predict
    vec = vectorizer.transform([clean_text])
    prediction = model.predict(vec)[0]

    print("📝 Your Review:", review_text)
    print("🔮 Predicted Sentiment:", prediction)

# ✍️ Example usage
predict_review_sentiment("This product was really bad and a complete waste of money.")
predict_review_sentiment("Absolutely loved it! Will recommend to everyone.")
predict_review_sentiment("It was okay, nothing special.")

pip install gradio

import gradio as gr
import random
import re

# Simple preprocessing (adjust as per your model)
def preprocess(text):
    text = re.sub(r'[^a-zA-Z ]', '', text)
    words = text.lower().split()
    words = [stemmer.stem(w) for w in words if w not in stop_words]
    return " ".join(words)

# Generate burst-style flying emojis
def generate_emoji_explosion(emojis):
    html = "<style>"
    html += """
    @keyframes fly {
        0% { transform: translateY(0) scale(1); opacity: 0; }
        10% { opacity: 1; }
        100% { transform: translateY(-120vh) scale(1.5); opacity: 0; }
    }
    .emoji-burst {
        position: fixed;
        font-size: 2.5rem;
        animation: fly 4s ease-out forwards;
        z-index: 9999;
        pointer-events: none;
    }
    """
    html += "</style>"

    for _ in range(75):
        emoji = random.choice(emojis)
        left = random.randint(0, 100)
        bottom = random.randint(0, 20)  # random start height near bottom
        delay = round(random.uniform(0, 3), 2)
        html += (
            f"<span class='emoji-burst' style='left:{left}%; bottom:{bottom}%; "
            f"animation-delay:{delay}s'>{emoji}</span>\n"
        )

    return html

# Main prediction + animation
def predict_sentiment_burst(review_text):
    clean = preprocess(review_text)
    vec = vectorizer.transform([clean])
    pred = model.predict(vec)[0]

    emoji_dict = {
        "positive": ['🎉', '✨', '😍', '💖', '🥳','💕','😘','❤️','😎'],
        "neutral":  ['😐', '🤔', '🫤','😊','🤓','👍'],
        "negative": ['💩', '😡', '💔', '🤮', '😭','😒','🤦‍♀️','🤡','💢','😤']
    }

    flying_emojis = generate_emoji_explosion(emoji_dict.get(pred, ['❓']))
    return flying_emojis + f"<h2 style='text-align:center;'>Sentiment: {pred}</h2>"

# Gradio UI
iface = gr.Interface(
    fn=predict_sentiment_burst,
    inputs=gr.Textbox(lines=3, placeholder="Type your review here..."),
    outputs=gr.HTML(),
    title="🚀 Amazon Product Review Sentiment Analysis",
    description=""
)

iface.launch(share=True)